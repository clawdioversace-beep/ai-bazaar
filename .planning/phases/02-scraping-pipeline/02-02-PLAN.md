---
phase: 02-scraping-pipeline
plan: 02
type: execute
wave: 2
depends_on: ["02-01"]
files_modified:
  - src/scrapers/github-scraper.ts
  - src/scrapers/npm-scraper.ts
  - src/scrapers/huggingface-scraper.ts
  - .env.example
autonomous: true
user_setup:
  - service: github
    why: "GitHub API authentication for 5000 req/hour rate limit (vs 60 unauthenticated)"
    env_vars:
      - name: GITHUB_TOKEN
        source: "GitHub → Settings → Developer settings → Personal access tokens → Fine-grained tokens → Generate new token (public_repo read-only scope is sufficient)"
    dashboard_config: []
  - service: huggingface
    why: "Optional — increases HuggingFace API rate limit for model/space listing"
    env_vars:
      - name: HUGGINGFACE_TOKEN
        source: "HuggingFace → Settings → Access Tokens → New token (read-only scope)"
    dashboard_config: []

must_haves:
  truths:
    - "GitHub scraper fetches repos by topic using Octokit paginate, normalizes each via github-normalizer, and upserts via CatalogService"
    - "npm scraper fetches packages by keyword using registry search API, handles the 250-result-per-query cap by running multiple keyword queries"
    - "HuggingFace scraper fetches models and spaces by tag, handling both SDK and fallback direct-fetch approaches"
    - "Each scraper returns a count of processed entries and logs errors for individual failures without aborting the batch"
    - "Re-running any scraper on existing data updates entries in place without creating duplicates"
  artifacts:
    - path: "src/scrapers/github-scraper.ts"
      provides: "GitHub topic-based repo scraper using Octokit"
      exports: ["scrapeGitHub"]
    - path: "src/scrapers/npm-scraper.ts"
      provides: "npm keyword-based package scraper"
      exports: ["scrapeNpm"]
    - path: "src/scrapers/huggingface-scraper.ts"
      provides: "HuggingFace models/spaces tag scraper"
      exports: ["scrapeHuggingFace"]
    - path: ".env.example"
      provides: "Required and optional env vars for scraping"
      contains: "GITHUB_TOKEN"
  key_links:
    - from: "src/scrapers/github-scraper.ts"
      to: "src/scrapers/normalizers/github-normalizer.ts"
      via: "import normalizeGitHubRepo"
      pattern: "import.*normalizeGitHubRepo.*from.*normalizers"
    - from: "src/scrapers/github-scraper.ts"
      to: "src/services/catalog.ts"
      via: "import upsertBySourceUrl"
      pattern: "import.*upsertBySourceUrl.*from.*services/catalog"
    - from: "src/scrapers/npm-scraper.ts"
      to: "src/scrapers/normalizers/npm-normalizer.ts"
      via: "import normalizeNpmPackage"
      pattern: "import.*normalizeNpmPackage.*from.*normalizers"
    - from: "src/scrapers/npm-scraper.ts"
      to: "src/services/catalog.ts"
      via: "import upsertBySourceUrl"
      pattern: "import.*upsertBySourceUrl.*from.*services/catalog"
    - from: "src/scrapers/huggingface-scraper.ts"
      to: "src/scrapers/normalizers/huggingface-normalizer.ts"
      via: "import normalizeHuggingFaceEntry"
      pattern: "import.*normalizeHuggingFaceEntry.*from.*normalizers"
    - from: "src/scrapers/huggingface-scraper.ts"
      to: "src/services/catalog.ts"
      via: "import upsertBySourceUrl"
      pattern: "import.*upsertBySourceUrl.*from.*services/catalog"
---

<objective>
Build the three API scrapers (GitHub, npm, HuggingFace) that fetch data from external APIs, normalize it through Plan 02-01's normalizers, and upsert into the catalog via CatalogService.

Purpose: These scrapers are the data pipeline that fills the catalog. Each is a standalone async function that can be called by the worker/scheduler in Plan 02-03 or directly by the seed script.

Output: 3 scraper files + updated .env.example. Install octokit and @huggingface/hub dependencies.
</objective>

<execution_context>
@/Users/clawdioversace/.claude/get-shit-done/workflows/execute-plan.md
@/Users/clawdioversace/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/02-scraping-pipeline/02-RESEARCH.md
@.planning/phases/02-scraping-pipeline/02-01-SUMMARY.md
@src/services/catalog.ts
@src/lib/catalog-schema.ts
</context>

<tasks>

<task type="auto">
  <name>Task 1: Install dependencies and create GitHub scraper</name>
  <files>src/scrapers/github-scraper.ts, .env.example</files>
  <action>
**Step 1: Install dependencies**
```bash
bun add octokit @huggingface/hub
```

**Step 2: Update `.env.example`** to include:
```
# GitHub Personal Access Token (required for scraping — 5000 req/hour vs 60 unauthenticated)
# Generate at: GitHub → Settings → Developer settings → Personal access tokens
GITHUB_TOKEN=ghp_your_token_here

# HuggingFace Access Token (optional — increases rate limit)
# Generate at: huggingface.co → Settings → Access Tokens
HUGGINGFACE_TOKEN=hf_your_token_here

# Turso database URL (existing from Phase 1)
TURSO_DATABASE_URL=file:./dev.db
```

**Step 3: Create `src/scrapers/github-scraper.ts`**

```typescript
import { Octokit } from 'octokit';
import { normalizeGitHubRepo } from './normalizers/github-normalizer';
import { upsertBySourceUrl } from '../services/catalog';
```

Export a single function:
```typescript
export async function scrapeGitHub(topic: string): Promise<{ processed: number; errors: number }>
```

**Implementation details:**
- Create Octokit instance: `new Octokit({ auth: process.env.GITHUB_TOKEN })` — Bun loads .env automatically
- If `GITHUB_TOKEN` is not set, log a warning but continue (unauthenticated gets 60 req/hour which is enough for testing)
- Use `octokit.paginate.iterator()` for GitHub search repos API:
  ```typescript
  const iterator = octokit.paginate.iterator(
    octokit.rest.search.repos,
    { q: `topic:${topic}`, sort: 'stars', order: 'desc', per_page: 100 }
  );
  ```
- Iterate pages: `for await (const { data: repos } of iterator)` — Octokit returns items in `data` array
- For each repo, wrap in try/catch:
  1. Call `normalizeGitHubRepo(repo)` to get CatalogEntryInput
  2. Call `await upsertBySourceUrl(entry)` to insert/update
  3. Increment `processed` counter
  4. On catch: log error with `console.error(\`[github-scraper] Failed: \${repo.full_name}: \${err}\`)`, increment `errors` counter
- After all pages: log summary `console.log(\`[github-scraper] topic=\${topic}: \${processed} processed, \${errors} errors\`)`
- Return `{ processed, errors }`

**Rate limiting note:** Octokit handles primary rate limit detection automatically (it reads X-RateLimit headers and waits). Do NOT add additional sleep between requests — Octokit's built-in throttling is sufficient. Secondary rate limits (403 with retry-after) are also handled by Octokit's retry plugin if installed, but for now rely on the 100-per-page pagination being naturally slow enough.

**Cap results:** Add an optional `maxResults` parameter (default 500) to stop pagination after N repos processed. This prevents runaway scrapes:
```typescript
export async function scrapeGitHub(
  topic: string,
  maxResults = 500
): Promise<{ processed: number; errors: number }>
```
Break out of the outer pagination loop when `processed + errors >= maxResults`.
  </action>
  <verify>
1. `bunx tsc --noEmit` — no TypeScript errors
2. Manual smoke test (if GITHUB_TOKEN is set): `TURSO_DATABASE_URL=file:./dev.db bun -e "import { scrapeGitHub } from './src/scrapers/github-scraper'; scrapeGitHub('mcp-server').then(r => console.log(r))"` — should output `{ processed: N, errors: M }` where N > 0
3. If GITHUB_TOKEN is not set: verify it logs a warning and doesn't crash
  </verify>
  <done>GitHub scraper fetches repos by topic, normalizes via github-normalizer, upserts via CatalogService, and returns processed/error counts</done>
</task>

<task type="auto">
  <name>Task 2: npm registry scraper</name>
  <files>src/scrapers/npm-scraper.ts</files>
  <action>
Create `src/scrapers/npm-scraper.ts`.

```typescript
import { fetchWithRetry } from '../lib/fetch-with-retry';
import { normalizeNpmPackage } from './normalizers/npm-normalizer';
import { upsertBySourceUrl } from '../services/catalog';
```

Export:
```typescript
export async function scrapeNpm(
  keyword: string,
  maxResults = 250
): Promise<{ processed: number; errors: number }>
```

**Implementation:**
- npm search API endpoint: `https://registry.npmjs.org/-/v1/search?text=keywords:${encodeURIComponent(keyword)}&size=250&from=0`
- Use `fetchWithRetry()` for the HTTP call with `{ maxAttempts: 3, baseDelay: 1000, timeout: 15000 }`
- Parse response as JSON: `const data = await response.json()`
- The response shape is: `{ objects: [{ package: { name, version, description, keywords, links, publisher } }], total: number }`
- For each `item` in `data.objects`:
  1. Call `normalizeNpmPackage(item.package)` — wrap in try/catch
  2. Call `await upsertBySourceUrl(entry)`
  3. Increment `processed` or `errors` counter
- npm caps at 250 results per query — do NOT attempt pagination beyond this (research Pitfall 2)
- Log summary: `console.log(\`[npm-scraper] keyword=${keyword}: ${processed} processed, ${errors} errors\`)`
- Return `{ processed, errors }`

**Edge cases:**
- If `response.ok` is false, throw with status code
- If `data.objects` is empty, return `{ processed: 0, errors: 0 }` (valid result — no packages match)
- Normalize errors from Zod parse failures should be caught and counted, not thrown
  </action>
  <verify>
1. `bunx tsc --noEmit` — no TypeScript errors
2. Manual smoke test: `TURSO_DATABASE_URL=file:./dev.db bun -e "import { scrapeNpm } from './src/scrapers/npm-scraper'; scrapeNpm('mcp').then(r => console.log(r))"` — should output `{ processed: N, errors: M }` where N > 0
  </verify>
  <done>npm scraper fetches packages by keyword, normalizes via npm-normalizer, upserts via CatalogService, handles the 250-result cap correctly</done>
</task>

<task type="auto">
  <name>Task 3: HuggingFace Hub scraper</name>
  <files>src/scrapers/huggingface-scraper.ts</files>
  <action>
Create `src/scrapers/huggingface-scraper.ts`.

**Important research finding (Pitfall 3):** The @huggingface/hub TypeScript SDK tag filtering syntax is uncertain. The SDK exports `listModels` and `listSpaces` iterators, but tag-based filtering may differ from the Python SDK. Implement a two-strategy approach:

**Strategy 1 (preferred): Try @huggingface/hub SDK**
```typescript
import { listModels, listSpaces } from '@huggingface/hub';
```

Try using:
```typescript
for await (const model of listModels({ search: { query: tag } })) { ... }
```

The `search` parameter on `listModels` accepts `{ query?: string }` — use the tag as the query string since tag-specific filtering may not be available.

**Strategy 2 (fallback): Direct API fetch**
If SDK approach returns no results or errors, fall back to direct HuggingFace API:
```
GET https://huggingface.co/api/models?search=${tag}&limit=100
GET https://huggingface.co/api/spaces?search=${tag}&limit=100
```
Use `fetchWithRetry()` for these calls.

**Export:**
```typescript
export async function scrapeHuggingFace(
  tag: string,
  maxResults = 200
): Promise<{ processed: number; errors: number }>
```

**Implementation:**
1. Set `accessToken` from `process.env.HUGGINGFACE_TOKEN` (optional — log if missing but don't crash)
2. Try SDK approach first for models (`listModels`)
3. For each model, call `normalizeHuggingFaceEntry(model)` then `upsertBySourceUrl(entry)`
4. Then scrape spaces (`listSpaces`) with same tag
5. Count processed and errors separately
6. If SDK yields 0 results, attempt fallback direct API
7. Cap at `maxResults` total entries

**Rate limiting:** HuggingFace has no documented rate limit for API reads, but be conservative. If the SDK handles pagination, let it. For direct fetch, use `fetchWithRetry` defaults.

Log: `console.log(\`[huggingface-scraper] tag=${tag}: ${processed} processed, ${errors} errors\`)`

Return `{ processed, errors }`
  </action>
  <verify>
1. `bunx tsc --noEmit` — no TypeScript errors
2. Manual smoke test: `TURSO_DATABASE_URL=file:./dev.db bun -e "import { scrapeHuggingFace } from './src/scrapers/huggingface-scraper'; scrapeHuggingFace('agent').then(r => console.log(r))"` — should output `{ processed: N, errors: M }`
  </verify>
  <done>HuggingFace scraper fetches models and spaces by tag with SDK + direct API fallback, normalizes via huggingface-normalizer, upserts via CatalogService</done>
</task>

</tasks>

<verification>
1. `bunx tsc --noEmit` — zero TypeScript errors across all scraper files
2. `bun test` — all existing tests still pass (no regressions)
3. Each scraper can be run independently via `bun -e "import ..."` smoke test
4. `.env.example` documents all required and optional env vars
5. `package.json` includes `octokit` and `@huggingface/hub` in dependencies
</verification>

<success_criteria>
- All 3 scrapers compile, export their main function, and handle errors gracefully
- GitHub scraper paginates via Octokit and respects maxResults cap
- npm scraper handles 250-result cap and uses fetchWithRetry
- HuggingFace scraper has SDK + fallback approach for resilience
- Re-running any scraper on same data updates entries (dedup proven by upsertBySourceUrl)
- No hardcoded API tokens — all from env vars
</success_criteria>

<output>
After completion, create `.planning/phases/02-scraping-pipeline/02-02-SUMMARY.md`
</output>
