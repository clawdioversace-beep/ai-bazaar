---
phase: 02-scraping-pipeline
plan: 03
type: execute
wave: 3
depends_on: ["02-02"]
files_modified:
  - src/scripts/seed-catalog.ts
  - src/workers/scrape-worker.ts
  - src/jobs/schedule-scrapes.ts
  - package.json
autonomous: true

must_haves:
  truths:
    - "Running `bun src/scripts/seed-catalog.ts` against a fresh database produces 200+ catalog entries across 5+ categories"
    - "Re-running the seed script on existing data does not create duplicates (entry count stays stable or grows only from new upstream data)"
    - "The scrape worker processes jobs from a bunqueue and invokes the correct scraper per source type"
    - "The scheduler enqueues daily re-index jobs for all 3 sources and a dead-link check job"
    - "Dead-link check iterates all listings and calls checkDeadLink + markDeadLink for each"
  artifacts:
    - path: "src/scripts/seed-catalog.ts"
      provides: "One-time pre-seed script that runs all scrapers with predefined topics/keywords"
      exports: []
    - path: "src/workers/scrape-worker.ts"
      provides: "bunqueue worker process that dispatches scrape jobs to appropriate scrapers"
      exports: ["startWorkers", "stopWorkers"]
    - path: "src/jobs/schedule-scrapes.ts"
      provides: "Cron scheduler that enqueues daily scrape and dead-link check jobs"
      exports: ["scheduleAllJobs"]
  key_links:
    - from: "src/scripts/seed-catalog.ts"
      to: "src/scrapers/github-scraper.ts"
      via: "import scrapeGitHub"
      pattern: "import.*scrapeGitHub"
    - from: "src/scripts/seed-catalog.ts"
      to: "src/scrapers/npm-scraper.ts"
      via: "import scrapeNpm"
      pattern: "import.*scrapeNpm"
    - from: "src/scripts/seed-catalog.ts"
      to: "src/scrapers/huggingface-scraper.ts"
      via: "import scrapeHuggingFace"
      pattern: "import.*scrapeHuggingFace"
    - from: "src/scripts/seed-catalog.ts"
      to: "src/services/search.ts"
      via: "import rebuildFtsIndex for post-seed index rebuild"
      pattern: "import.*rebuildFtsIndex"
    - from: "src/workers/scrape-worker.ts"
      to: "src/scrapers/github-scraper.ts"
      via: "dispatches github jobs to scrapeGitHub"
      pattern: "scrapeGitHub"
    - from: "src/jobs/schedule-scrapes.ts"
      to: "src/workers/scrape-worker.ts"
      via: "enqueues jobs that workers process"
      pattern: "Queue.*add"
---

<objective>
Build the pre-seed script, bunqueue worker process, and cron scheduler. Run the seed script to pre-populate the catalog with 200+ entries across 5+ categories, satisfying the Phase 2 success criteria.

Purpose: This plan ties together all scrapers into an operational pipeline. The seed script proves end-to-end functionality; the worker and scheduler enable automated re-indexing.

Output: 3 files + updated package.json scripts. Running seed populates the catalog to the 200+ entry threshold.
</objective>

<execution_context>
@/Users/clawdioversace/.claude/get-shit-done/workflows/execute-plan.md
@/Users/clawdioversace/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/02-scraping-pipeline/02-RESEARCH.md
@.planning/phases/02-scraping-pipeline/02-01-SUMMARY.md
@.planning/phases/02-scraping-pipeline/02-02-SUMMARY.md
@src/services/catalog.ts
@src/services/search.ts
</context>

<tasks>

<task type="auto">
  <name>Task 1: Pre-seed script and package.json scripts</name>
  <files>src/scripts/seed-catalog.ts, package.json</files>
  <action>
Create `src/scripts/seed-catalog.ts` — a standalone script that runs all scrapers with predefined topics/keywords to pre-populate the catalog.

**This is the PRIMARY deliverable of Phase 2.** It must produce 200+ entries across 5+ categories.

```typescript
import { scrapeGitHub } from '../scrapers/github-scraper';
import { scrapeNpm } from '../scrapers/npm-scraper';
import { scrapeHuggingFace } from '../scrapers/huggingface-scraper';
import { rebuildFtsIndex } from '../services/search';
import { getAllListings } from '../services/catalog';
```

**Topic/keyword configuration (designed to hit 200+ entries):**

GitHub topics (4 searches, expect ~100-200 unique repos per topic with overlap):
- `mcp-server` — MCP protocol servers
- `ai-agent` — AI agent frameworks and tools
- `model-context-protocol` — MCP-related projects
- `web3` — Web3/blockchain tools (cap at 200 to focus on relevant results)

npm keywords (4 searches, expect ~50-200 per keyword):
- `mcp` — MCP packages
- `ai-agent` — AI agent packages
- `web3` — Web3 packages
- `agent-framework` — Agent framework packages

HuggingFace tags (3 searches, expect ~30-100 per tag):
- `agent` — Agent models/spaces
- `web3` — Web3-related models
- `mcp` — MCP models (may return few results)

**Script flow:**
1. Log start time and banner: `console.log('=== AI Bazaar Catalog Seed ===')`
2. Run GitHub scrapes sequentially (to respect rate limits):
   ```typescript
   const githubTopics = ['mcp-server', 'ai-agent', 'model-context-protocol', 'web3'];
   for (const topic of githubTopics) {
     const result = await scrapeGitHub(topic, 300);
     totalResults.github += result.processed;
     totalErrors.github += result.errors;
   }
   ```
3. Run npm scrapes sequentially:
   ```typescript
   const npmKeywords = ['mcp', 'ai-agent', 'web3', 'agent-framework'];
   for (const keyword of npmKeywords) {
     const result = await scrapeNpm(keyword, 250);
     totalResults.npm += result.processed;
     totalErrors.npm += result.errors;
   }
   ```
4. Run HuggingFace scrapes sequentially:
   ```typescript
   const hfTags = ['agent', 'web3', 'mcp'];
   for (const tag of hfTags) {
     const result = await scrapeHuggingFace(tag, 200);
     totalResults.hf += result.processed;
     totalErrors.hf += result.errors;
   }
   ```
5. Rebuild FTS5 index: `await rebuildFtsIndex()` — critical after bulk inserts per Phase 1 pattern
6. Get final count: `const allListings = await getAllListings(1, 0)` — we just need the count, but since there's no `count()` function, use the existing `countByCategory()` from search service or just call `getAllListings(10000, 0)` and check `.length`

   Actually, better: import `countByCategory` from search service and sum counts:
   ```typescript
   import { countByCategory } from '../services/search';
   const categoryCounts = await countByCategory();
   const totalEntries = categoryCounts.reduce((sum, c) => sum + c.count, 0);
   const uniqueCategories = categoryCounts.filter(c => c.count > 0).length;
   ```

7. Print summary:
   ```
   === Seed Complete ===
   GitHub: {N} processed, {M} errors
   npm: {N} processed, {M} errors
   HuggingFace: {N} processed, {M} errors

   Total catalog entries: {N}
   Categories with entries: {N}

   Category breakdown:
     mcp-server: {N}
     ai-agent: {N}
     web3-tool: {N}
     ...
   ```

8. Validate success criteria:
   ```typescript
   if (totalEntries < 200) {
     console.error(`WARNING: Only ${totalEntries} entries (target: 200+)`);
     process.exit(1);
   }
   if (uniqueCategories < 5) {
     console.error(`WARNING: Only ${uniqueCategories} categories (target: 5+)`);
     process.exit(1);
   }
   console.log('SUCCESS: Catalog meets pre-seed targets');
   ```

**Add package.json scripts:**
```json
"seed": "TURSO_DATABASE_URL=file:./dev.db bun src/scripts/seed-catalog.ts",
"seed:prod": "bun src/scripts/seed-catalog.ts"
```
The `seed` script uses local dev.db; `seed:prod` expects TURSO_DATABASE_URL set in environment.

**Error handling:** The seed script should NOT crash on individual scraper failures. Each scraper is wrapped in try/catch at the top level too:
```typescript
try {
  const result = await scrapeGitHub(topic, 300);
  ...
} catch (err) {
  console.error(`[seed] GitHub scrape failed for topic=${topic}:`, err);
  totalErrors.github++;
}
```
This ensures that if one source is down (e.g., GitHub API rate limited), the others still run.
  </action>
  <verify>
1. Run the seed: `bun run seed` — should produce 200+ entries
2. Run seed again to verify idempotency: count should stay approximately the same (new upstream entries are fine, but no duplicates)
3. Check category distribution: `TURSO_DATABASE_URL=file:./dev.db bun -e "import { countByCategory } from './src/services/search'; countByCategory().then(r => console.log(r))"` — should show 5+ categories with entries
  </verify>
  <done>Seed script produces 200+ catalog entries across 5+ categories in a fresh database; re-running does not create duplicates; FTS5 index rebuilt after bulk insert</done>
</task>

<task type="auto">
  <name>Task 2: bunqueue worker and scheduler</name>
  <files>src/workers/scrape-worker.ts, src/jobs/schedule-scrapes.ts, package.json</files>
  <action>
**Step 1: Install bunqueue**
```bash
bun add bunqueue
```

**Step 2: Create `src/workers/scrape-worker.ts`**

This is the background worker that processes scrape jobs from bunqueue queues.

```typescript
import { Queue, Worker } from 'bunqueue';
```

Note: Research shows `from 'bunqueue/client'` — check the actual package export. It may be `from 'bunqueue'` directly. Read `node_modules/bunqueue/package.json` to verify the correct import path.

**Define 4 queues:**
1. `scrape-github` — GitHub topic scrape jobs
2. `scrape-npm` — npm keyword scrape jobs
3. `scrape-huggingface` — HuggingFace tag scrape jobs
4. `check-dead-links` — Dead link health checks

**Workers:**
Each worker processes jobs by calling the corresponding scraper:

```typescript
const githubWorker = new Worker('scrape-github', async (job) => {
  const { topic, maxResults } = job.data;
  return scrapeGitHub(topic, maxResults ?? 500);
}, { concurrency: 1 }); // Serial to respect rate limits

const npmWorker = new Worker('scrape-npm', async (job) => {
  const { keyword, maxResults } = job.data;
  return scrapeNpm(keyword, maxResults ?? 250);
}, { concurrency: 1 });

const hfWorker = new Worker('scrape-huggingface', async (job) => {
  const { tag, maxResults } = job.data;
  return scrapeHuggingFace(tag, maxResults ?? 200);
}, { concurrency: 1 });
```

**Dead link worker:**
```typescript
const deadLinkWorker = new Worker('check-dead-links', async (job) => {
  const { checkDeadLink, markDeadLink, getAllListings } = await import('../services/catalog');
  const listings = await getAllListings(10000, 0);
  let checked = 0, dead = 0;

  for (const listing of listings) {
    const isDead = await checkDeadLink(listing.sourceUrl);
    if (isDead) {
      await markDeadLink(listing.id, true);
      dead++;
    }
    checked++;
    // Throttle: max 10 checks/second
    if (checked % 10 === 0) await Bun.sleep(1000);
  }

  return { checked, dead };
}, { concurrency: 1 });
```

**Export start/stop functions:**
```typescript
export async function startWorkers() { ... } // Initialize all workers
export async function stopWorkers() { ... }  // Graceful shutdown
```

**Graceful shutdown:** Listen for SIGTERM and SIGINT:
```typescript
const shutdown = async (signal: string) => {
  console.log(`[scrape-worker] Received ${signal}, shutting down...`);
  await stopWorkers();
  process.exit(0);
};
process.on('SIGTERM', () => shutdown('SIGTERM'));
process.on('SIGINT', () => shutdown('SIGINT'));
```

**Step 3: Create `src/jobs/schedule-scrapes.ts`**

Export a function that enqueues recurring jobs:

```typescript
export async function scheduleAllJobs(): Promise<void>
```

Schedule:
- GitHub scrape: daily at 2 AM UTC — one job per topic (4 jobs)
- npm scrape: daily at 3 AM UTC — one job per keyword (4 jobs)
- HuggingFace scrape: daily at 4 AM UTC — one job per tag (3 jobs)
- Dead link check: daily at 5 AM UTC — single job

Use bunqueue's `repeat` option:
```typescript
await githubQueue.add('scrape', { topic: 'mcp-server' }, {
  repeat: { pattern: '0 2 * * *' }
});
```

After scheduling, log all scheduled jobs.

**Step 4: Update package.json scripts:**
```json
"worker": "TURSO_DATABASE_URL=file:./dev.db bun src/workers/scrape-worker.ts",
"worker:prod": "bun src/workers/scrape-worker.ts",
"schedule": "TURSO_DATABASE_URL=file:./dev.db bun src/jobs/schedule-scrapes.ts"
```

**Important bunqueue caveat:** bunqueue is relatively new. If the import path or API doesn't match research examples, adapt to the actual package exports. Read `node_modules/bunqueue/README.md` or `package.json` to verify API surface before implementing. If bunqueue API is significantly different from research patterns, simplify: use a minimal job dispatch pattern without queues for now, and leave a TODO for full queue integration.
  </action>
  <verify>
1. `bunx tsc --noEmit` — no TypeScript errors
2. `bun test` — all existing tests still pass
3. Worker file exports `startWorkers` and `stopWorkers` functions
4. Scheduler file exports `scheduleAllJobs` function
5. Smoke test worker: `TURSO_DATABASE_URL=file:./dev.db timeout 5 bun src/workers/scrape-worker.ts || true` — starts without errors, exits on timeout
  </verify>
  <done>bunqueue worker dispatches scrape jobs to correct scrapers; scheduler enqueues daily re-index + dead-link check jobs; graceful shutdown handles SIGTERM/SIGINT</done>
</task>

</tasks>

<verification>
1. `bun run seed` produces 200+ entries across 5+ categories (Phase 2 Success Criterion 1)
2. Running `bun run seed` a second time does not increase entry count significantly — dedup works (Success Criterion 2)
3. `bun test` — all tests pass (Phase 1 + Phase 2)
4. `bunx tsc --noEmit` — zero TypeScript errors
5. Category distribution check shows entries in at least: mcp-server, ai-agent, web3-tool, framework, and one more
6. Worker and scheduler compile and can be started without errors
</verification>

<success_criteria>
- 200+ catalog entries exist after seed run (measurable: count > 200)
- 5+ categories have at least 1 entry each (measurable: countByCategory shows 5+ non-zero)
- Duplicate run stability: second seed run produces ~same count (+/- new upstream entries only)
- Worker process starts, handles SIGTERM gracefully, and exits cleanly
- Scheduler enqueues recurring jobs for all sources + dead-link checks
- All package.json scripts work: seed, worker, schedule
</success_criteria>

<output>
After completion, create `.planning/phases/02-scraping-pipeline/02-03-SUMMARY.md`
</output>
