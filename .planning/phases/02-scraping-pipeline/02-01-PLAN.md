---
phase: 02-scraping-pipeline
plan: 01
type: tdd
wave: 1
depends_on: []
files_modified:
  - src/lib/fetch-with-retry.ts
  - src/scrapers/normalizers/github-normalizer.ts
  - src/scrapers/normalizers/npm-normalizer.ts
  - src/scrapers/normalizers/huggingface-normalizer.ts
  - src/lib/__tests__/fetch-with-retry.test.ts
  - src/lib/__tests__/normalizers.test.ts
autonomous: true

must_haves:
  truths:
    - "fetchWithRetry retries on 5xx and 429 with exponential backoff + jitter, and passes through 4xx (non-429) without retry"
    - "GitHub normalizer transforms a raw GitHub API repo object to a valid CatalogEntryInput with correct category, runtime, and slug"
    - "npm normalizer transforms a raw npm package object to a valid CatalogEntryInput preferring repository URL over npm page URL"
    - "HuggingFace normalizer transforms a raw HF model/space object to a valid CatalogEntryInput with huggingface.co source URL"
    - "All normalizers produce output that passes CatalogEntrySchema.parse() without errors"
  artifacts:
    - path: "src/lib/fetch-with-retry.ts"
      provides: "HTTP fetch wrapper with retry, backoff, jitter, and manual AbortController timeout"
      exports: ["fetchWithRetry"]
    - path: "src/scrapers/normalizers/github-normalizer.ts"
      provides: "GitHub repo to CatalogEntryInput transform"
      exports: ["normalizeGitHubRepo", "GitHubRepoSchema"]
    - path: "src/scrapers/normalizers/npm-normalizer.ts"
      provides: "npm package to CatalogEntryInput transform"
      exports: ["normalizeNpmPackage", "NpmPackageSchema"]
    - path: "src/scrapers/normalizers/huggingface-normalizer.ts"
      provides: "HuggingFace model/space to CatalogEntryInput transform"
      exports: ["normalizeHuggingFaceEntry", "HuggingFaceEntrySchema"]
    - path: "src/lib/__tests__/fetch-with-retry.test.ts"
      provides: "Unit tests for fetchWithRetry retry logic"
    - path: "src/lib/__tests__/normalizers.test.ts"
      provides: "Unit tests for all 3 normalizers"
  key_links:
    - from: "src/scrapers/normalizers/github-normalizer.ts"
      to: "src/lib/catalog-schema.ts"
      via: "imports CatalogEntryInput type and createSlug"
      pattern: "import.*CatalogEntryInput.*from.*catalog-schema"
    - from: "src/scrapers/normalizers/npm-normalizer.ts"
      to: "src/lib/catalog-schema.ts"
      via: "imports CatalogEntryInput type and createSlug"
      pattern: "import.*CatalogEntryInput.*from.*catalog-schema"
    - from: "src/scrapers/normalizers/huggingface-normalizer.ts"
      to: "src/lib/catalog-schema.ts"
      via: "imports CatalogEntryInput type and createSlug"
      pattern: "import.*CatalogEntryInput.*from.*catalog-schema"
---

<objective>
Build the shared HTTP retry utility and all three source-specific normalizers (GitHub, npm, HuggingFace) as pure, testable functions with full unit test coverage.

Purpose: These are the foundation modules that all scrapers depend on. By building and testing them first, scraper implementation (Plan 02-02) becomes a simple wiring exercise.

Output: 4 source files + 2 test files, all passing `bun test`.
</objective>

<execution_context>
@/Users/clawdioversace/.claude/get-shit-done/workflows/execute-plan.md
@/Users/clawdioversace/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/02-scraping-pipeline/02-RESEARCH.md
@src/lib/catalog-schema.ts
@src/lib/categories.ts
@src/lib/tags.ts
</context>

<tasks>

<task type="auto">
  <name>Task 1: fetchWithRetry utility with tests</name>
  <files>src/lib/fetch-with-retry.ts, src/lib/__tests__/fetch-with-retry.test.ts</files>
  <action>
Create `src/lib/fetch-with-retry.ts` implementing the fetchWithRetry function from research Pattern 4.

**Interface:**
```typescript
interface RetryOptions {
  maxAttempts?: number;   // default 3
  baseDelay?: number;     // default 1000ms
  maxDelay?: number;      // default 30000ms
  timeout?: number;       // default 10000ms
}

export async function fetchWithRetry(
  url: string,
  options?: RequestInit,
  retryOptions?: RetryOptions
): Promise<Response>
```

**Behavior:**
- Uses manual `new AbortController()` + `setTimeout()` for timeout (NOT `AbortSignal.timeout()` — Bun bug per research)
- Retries ONLY on: HTTP 429 (rate limit) and 5xx (server error)
- Does NOT retry on: 4xx (non-429) — returns response immediately
- Checks `Retry-After` header on 429 responses and respects it (parse as seconds -> ms)
- Exponential backoff: `baseDelay * 2^(attempt-1)`, capped at `maxDelay`
- Jitter: add `Math.random() * exponentialDelay` to the delay
- Uses `Bun.sleep()` for delays (not `setTimeout` with Promise)
- Throws the last error after all attempts exhausted
- Always clears the AbortController timeout in finally block

**Tests (RED then GREEN):**
Create `src/lib/__tests__/fetch-with-retry.test.ts`:
1. Test that successful response returns immediately without retry
2. Test that 429 response triggers retry (mock fetch to return 429 then 200)
3. Test that 5xx response triggers retry (mock fetch to return 503 then 200)
4. Test that 4xx (non-429) response returns immediately without retry (e.g. 404)
5. Test that maxAttempts is respected (mock always-failing, verify attempt count)
6. Test that Retry-After header is respected (mock 429 with Retry-After: 1)

Use `bun test` with `import { test, expect, mock, spyOn } from 'bun:test'`. Mock global `fetch` using `mock()` from bun:test. Use `Bun.sleep` mock or time control if needed, but prefer fast tests by setting baseDelay to 1ms in test options.
  </action>
  <verify>
`bun test src/lib/__tests__/fetch-with-retry.test.ts` — all 6 tests pass
  </verify>
  <done>fetchWithRetry correctly retries on 429/5xx, respects Retry-After header, and passes through non-retryable errors</done>
</task>

<task type="auto">
  <name>Task 2: Source-specific normalizers with tests</name>
  <files>src/scrapers/normalizers/github-normalizer.ts, src/scrapers/normalizers/npm-normalizer.ts, src/scrapers/normalizers/huggingface-normalizer.ts, src/lib/__tests__/normalizers.test.ts</files>
  <action>
Create all three normalizer files and a shared test file.

**GitHub Normalizer (`src/scrapers/normalizers/github-normalizer.ts`):**
- Define `GitHubRepoSchema` using Zod to validate raw GitHub API search result shape (see research Pattern 1):
  - `full_name: string`, `description: string | null`, `html_url: string (url)`, `homepage: string | null` (not URL — GitHub returns empty string sometimes), `stargazers_count: number`, `topics: string[]`, `license: { spdx_id: string } | null`, `language: string | null`
- Export `normalizeGitHubRepo(repo: unknown): CatalogEntryInput` that parses through GitHubRepoSchema then maps:
  - `slug`: `createSlug(full_name)` — e.g. "modelcontextprotocol/servers" -> "modelcontextprotocol-servers"
  - `name`: `full_name`
  - `tagline`: `description?.slice(0, 160)` or fallback `"GitHub repository: ${full_name}"`
  - `description`: `description` or fallback `"No description provided for ${full_name}"`
  - `category`: Determine from topics — if any topic includes 'mcp' -> 'mcp-server'; if any topic in ['web3','blockchain','ethereum','solana','defi'] -> 'web3-tool'; if any in ['defi','yield','swap','amm'] -> 'defi-tool'; else 'ai-agent'
  - `tags`: `topics` (will be normalized by CatalogEntrySchema)
  - `sourceUrl`: `html_url`
  - `docsUrl`: `homepage` if truthy and valid URL (try new URL(), catch -> undefined)
  - `licenseType`: `license?.spdx_id` or undefined
  - `runtime`: Map from `language` — TypeScript/JavaScript -> 'node', Python -> 'python', Rust -> 'rust', Go -> 'go', others -> 'other', null -> undefined
  - `stars`: `stargazers_count`
  - `mcpCompatible`: true if any topic includes 'mcp'
  - `submittedBy`: `'github-scraper'`

**npm Normalizer (`src/scrapers/normalizers/npm-normalizer.ts`):**
- Define `NpmPackageSchema` using Zod for npm search result `.objects[].package` shape:
  - `name: string`, `version: string`, `description: string | undefined`, `keywords: string[] | undefined`, `links: { npm: string, homepage?: string, repository?: string }`, `publisher: { username: string }`
- Export `normalizeNpmPackage(pkg: unknown): CatalogEntryInput` that parses and maps:
  - `slug`: `createSlug(name)` — npm scoped packages like "@scope/name" -> "scope-name"
  - `name`: `name`
  - `tagline`: `description?.slice(0, 160)` or fallback
  - `description`: `description` or fallback
  - `category`: From keywords — mcp keywords -> 'mcp-server'; web3/blockchain/ethereum/solana -> 'web3-tool'; defi/yield -> 'defi-tool'; else 'ai-agent'
  - `tags`: `keywords ?? []`
  - `sourceUrl`: Prefer `links.repository` over `links.npm` (GitHub URL is more stable)
  - `docsUrl`: `links.homepage` if truthy
  - `runtime`: Always `'node'` (npm packages are Node.js ecosystem)
  - `submittedBy`: `'npm-scraper'`

**HuggingFace Normalizer (`src/scrapers/normalizers/huggingface-normalizer.ts`):**
- Define `HuggingFaceEntrySchema` using Zod for HF model/space shape:
  - `id: string` (e.g. "username/model-name"), `tags: string[] | undefined`, `downloads: number | undefined`, `likes: number | undefined`, `private: boolean | undefined`
  - Note: HF API returns various shapes; keep this loose and handle missing fields gracefully
- Export `normalizeHuggingFaceEntry(entry: unknown): CatalogEntryInput` that maps:
  - `slug`: `createSlug(id)` — "username/model-name" -> "username-model-name"
  - `name`: `id`
  - `tagline`: First tag-derived description or `"HuggingFace model: ${id}"`
  - `description`: `"HuggingFace model ${id} with tags: ${tags?.join(', ')}"` or fallback
  - `category`: From tags — 'agent' tag -> 'ai-agent'; 'web3'/'blockchain' -> 'web3-tool'; 'mcp' -> 'mcp-server'; else 'framework'
  - `tags`: `tags ?? []`
  - `sourceUrl`: `"https://huggingface.co/${id}"`
  - `stars`: `likes ?? 0`
  - `downloads`: `downloads ?? 0`
  - `submittedBy`: `'huggingface-scraper'`

**Test file (`src/lib/__tests__/normalizers.test.ts`):**
Create tests for all 3 normalizers using fixture data:

GitHub tests (3):
1. Valid repo with all fields → CatalogEntryInput passes CatalogEntrySchema.parse()
2. Repo with MCP topic → category is 'mcp-server', mcpCompatible is true
3. Repo with null description and license → uses fallbacks

npm tests (3):
1. Valid package with keywords and repository link → correct sourceUrl (repository preferred over npm page)
2. Package with MCP keyword → category is 'mcp-server'
3. Package with no keywords and no repository → uses npm link as sourceUrl, empty tags

HuggingFace tests (3):
1. Valid model → sourceUrl is `https://huggingface.co/${id}`
2. Model with 'agent' tag → category is 'ai-agent'
3. Model with no tags, no downloads → uses defaults gracefully

Every test should verify output passes `CatalogEntrySchema.parse(result)` without errors.
  </action>
  <verify>
`bun test src/lib/__tests__/normalizers.test.ts` — all 9 tests pass. Every normalizer output validates against CatalogEntrySchema.
  </verify>
  <done>All 3 normalizers transform raw API data to valid CatalogEntryInput; 9 tests prove correctness including edge cases (null fields, missing optional data, category detection from tags/topics)</done>
</task>

</tasks>

<verification>
1. `bun test src/lib/__tests__/fetch-with-retry.test.ts` — 6 tests pass
2. `bun test src/lib/__tests__/normalizers.test.ts` — 9 tests pass
3. `bun test` — all project tests pass (including existing Phase 1 tests)
4. TypeScript compiles with no errors: `bunx tsc --noEmit`
</verification>

<success_criteria>
- fetchWithRetry handles retry/no-retry correctly for all HTTP status categories
- All 3 normalizers produce CatalogEntrySchema-valid output from realistic API fixture data
- 15 total new tests pass alongside existing 7 Phase 1 tests
- No runtime dependencies added yet (normalizers are pure functions; Octokit/bunqueue come in later plans)
</success_criteria>

<output>
After completion, create `.planning/phases/02-scraping-pipeline/02-01-SUMMARY.md`
</output>
